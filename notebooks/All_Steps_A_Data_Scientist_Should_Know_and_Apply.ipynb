{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"0\"></a>\n",
    "# **V7.All Steps A Data Scientist Should Know and Apply**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"0.1\"></a>\n",
    "# **Table of Contents**\n",
    "\n",
    "**Part 1: Know the Tools**\n",
    "\n",
    "1. Python\n",
    "    - first session\n",
    "    - packs: numpy, pandas, scikitlearn, plotly\n",
    "\n",
    "\n",
    "2. Anaconda Platform and Data Science Tools \n",
    "    - Notebook\n",
    "    - Orange\n",
    "    - Github\n",
    "\n",
    "\n",
    "**Part 2: Touch the Data**\n",
    "      \n",
    "3. Touching the Data First Time\n",
    "    + import, read, data structure, id, indexing\n",
    "    + explore the dim (shape): small, large or high dimensional data\n",
    "    + categorize the types of measurement\n",
    "        - by column name\n",
    "        - combine by types: numerical, categorical\n",
    "    + univariate visualization as Exploratory Data Analysis (EDA) \n",
    "        - descriptives \n",
    "        - summaries\n",
    "    + bivariate visualization as EDA\n",
    "        - Numeric-Numeric: scatterplot, \n",
    "        - Numeric-Categorical: boxplot, multi boxplot\n",
    "        - Categorical-Categorical: bar chart, side-by-side bar, Mosaic, \n",
    "    + explore the distributional structure: univariate, bivariate, trivariate, and multivariate\n",
    "        - fit a known distribution (goodness of fit) to the data\n",
    "        - explore the distribution structure: univariate, bivariate, trivariate, and multivariate\n",
    "    + explore the Gaussian structure: univariate, bivariate, trivariate, and multivariate\n",
    "\n",
    "   \n",
    "4. Visualize the Data \n",
    "     + Underlying Structure of Data\n",
    "     + Multivariate Screening Tools\n",
    "     + Multivariate Visualization\n",
    "     + Data Cloud\n",
    "     + Contour, Big Data Tools\n",
    "     + Sparsed Data Analysis\n",
    "     + Which columns to drop\n",
    "\n",
    "\n",
    "**Part 3: Select the Models**\n",
    "\n",
    "\n",
    "5. Type of Problems and Goals\n",
    "    + Specific goals and the models to adress: connect to the data and the model\n",
    "    + Input and Output Spaces\n",
    "      - determine the data space: input, target/output\n",
    "      - architecture of models and variables: input-output, cause-effect     \n",
    "\n",
    "6. Simple Statistical Models\n",
    "    - introduction to hypothesis testing with p-value and CI \n",
    "    - ttest/ztest: two group case (mean and proportion problems)\n",
    "    - median test: two group case (mean problem)\n",
    "    - Chi-squared test (categorical situation, multi-sample proportion, goodness-of-fit)\n",
    "    - ANOVA, Kruskall-Wallis (multi-sample mean problem)\n",
    "    - Simple linear problem (response as categorical), correlation (with alternative metricss: Pearson, Spearman)\n",
    "    - Simple logistic regression (response as categorical)\n",
    "    - Other inferential methods\n",
    "    - Detect patterns \n",
    "    - Importance analysis\n",
    "    - Naive Cause-Effect Models\n",
    "\n",
    "\n",
    "7. Machine Learning Methods and Their Uses in a Nutshell \n",
    "    + Statistical \n",
    "    + Unsupervised\n",
    "    + Supervised\n",
    "    + Mix of un/supervised\n",
    "    \n",
    "**A Case Study\n",
    "\n",
    "\n",
    "\n",
    "**Part 4: Prepare the Data**\n",
    "\n",
    "\n",
    "8. Missing Data Exploration and Imputation Methods\n",
    "    + explore the missingness: column-wise and row-wise\n",
    "    + explore the missingness: bivariately\n",
    "    + categorize the missingness: \n",
    "        - Missing Completely at Random (MCAR):\n",
    "        - Missing at Random (MAR)\n",
    "        - Missing not at random (MNAR)\n",
    "    + catch the pattern and impute the missingness:\n",
    "        - numerical\n",
    "        - categorical\n",
    "        - correlation, association, differences\n",
    "        - use of heatmap, dendrogram, mosaic plots\n",
    "        - use of statistical methods to catch the pattern: \n",
    "            - p-values and test statistics from ttest, ztest, chi-squared values, ANOVA, correlation analysis, MLR, Logistic regression\n",
    "        - building imputed models\n",
    "    + write a function to generalize the missingness process for any data set\n",
    "\n",
    "\n",
    "9. Scaling Methods\n",
    "      + Normalization\n",
    "      + Standardization (Z-Score)\n",
    "      + Min-Max\n",
    "      + Scale to [a,b] \n",
    "      + Robust scaling: percentile, rank\n",
    "      + Other methods: Absolute value scaling, normalizer, other user defined\n",
    "\n",
    "\n",
    "10. Categorical Data scaling and Transformation\n",
    "    + Encoding methods [http://contrib.scikit-learn.org/categorical-encoding/index.html]\n",
    "    + Distributions, processed categorical data's continuous distributions\n",
    "    + Handling zero-inflated data\n",
    "\n",
    "\n",
    "11. Transformation\n",
    "    + six transformations in which classifier/model to be invariant: \n",
    "        + translation (two directions)\n",
    "        + scaling (two directions)\n",
    "        + sheer, and \n",
    "        + character thickness\n",
    "        + Mapping to a distribution\n",
    "    + linear transformation\n",
    "    + nonlinear transformation      \n",
    "    + Power transformation\n",
    "    + Density-based transformation\n",
    "    + Invariant Metrics and Tangent Distance\n",
    "    + Other Transformations\n",
    "        + GL algorithm\n",
    "        + Wavelet and Fourier transformations in clustering and signal processing\n",
    "\n",
    "\n",
    "12. Gaussian or Not [https://machinelearningmastery.com/probability-density-estimation/]\n",
    "    + Box-Cox transformation(want Gaussian looking)\n",
    "    + Normality check [https://machinelearningmastery.com/a-gentle-introduction-to-normality-tests-in-python/]\n",
    "\n",
    "**A Case Study\n",
    "\n",
    "\n",
    "\n",
    "**Part 5: Engineer the Features** \n",
    "[https://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114]\n",
    "\n",
    "(Apply 0. EDA, 1.Imputation, 2.Encoding, 3.Scaling, 4.Transform, 5.Data reduction, 6.Feature Selection, and 7.Feature Extraction before building models.)\n",
    "\n",
    "\n",
    "13. Feature Selection\n",
    "    + Filter\n",
    "    + Wrapper\n",
    "    + Embedded\n",
    "    + Simple models for numerical and categorical data on response: use of chisq, logistic, regression\n",
    "    + Automating ways for high dim data\n",
    "    + Grouping and tips\n",
    "    \n",
    "    \n",
    "14. Feature Extraction\n",
    "    + Encoding categorical features\n",
    "    + Discretization\n",
    "    + Image processing\n",
    "\n",
    "\n",
    "**A Case Study\n",
    "\n",
    "\n",
    "\n",
    "**Part 6: Build the Models**\n",
    "\n",
    "15. Model Building Steps\n",
    "    - Steps\n",
    "    - Validation\n",
    "    - Testing\n",
    "    - Scores/Metrics\n",
    "\n",
    "\n",
    "16. Supervised ML models (all basic models)\n",
    "\n",
    "      - Numerical response\n",
    "          - MLR, Nonlinear regression (Spline, GAM)\n",
    "          - Naive Bayes, kNN, SVM, NN\n",
    "          - Tree-based methods\n",
    "          - With constraints: Ridge, Lasso, Mix of L2 and L1, Other shrinkage/constraints\n",
    "          - Robust methods\n",
    "          \n",
    "      - Categorical response\n",
    "          - Logistic regression, LDA-QDA, kNN, SVM, NN, ...\n",
    "          - Tree-based methods\n",
    "          - With constraints: Ridge, Lasso, Mix of L2 and L1, Other shrinkage/constraints\n",
    "          - Robust methods\n",
    "          - Correspondence Analysis\n",
    "\n",
    "      - Algortihms, Boosted and advanceds models: numerical and categorical response\n",
    "          - Bagging, Random Forest, Boosting, Stacking\n",
    "          - Algorithms and tools: AdaBoost, XGBoost\n",
    "\n",
    "\n",
    "17. Unsupervised ML models (all basic models)\n",
    "\n",
    "    - PCA\n",
    "    - Distances\n",
    "    - Hierarchical Clustering\n",
    "    - k-Means\n",
    "    - Manifold Learning\n",
    "\n",
    "\n",
    "18. Deep Learning Methods\n",
    "    - Some architextures\n",
    "        - CNN\n",
    "        - RNN\n",
    "        - LSTM and GRU\n",
    "        - Attention Network\n",
    "        - Autoencoder\n",
    "    - Optimization Algorithms (open the black-box)\n",
    "        -Gradient descent optimization algorithms\n",
    "         -   Momentum\n",
    "         -   Nesterov accelerated gradient\n",
    "         -   Adagrad\n",
    "         -   RMSprop\n",
    "         -   Adam\n",
    "         -   AdaMax\n",
    "         -   Nadam\n",
    "    - Activation Functions\n",
    "        1. Binary Step\n",
    "        2. Linear\n",
    "        3. Sigmoid\n",
    "        4. Tanh\n",
    "        5. ReLU, ReLu6\n",
    "        6. Leaky ReLU\n",
    "        7. Parameterised ReLU\n",
    "        8. Exponential Linear Unit\n",
    "        9. Swish\n",
    "        10. Softmax\n",
    "\n",
    "    - Hyperparameters and Search Tips\n",
    "        - ...\n",
    "\n",
    "**A Case Study\n",
    "\n",
    "\n",
    "19. Advanced Statistical and Probabilistic Models\n",
    "    - Probability\n",
    "        - basics\n",
    "        - multivariate representations\n",
    "        - pdf, cdf\n",
    "        - useful theorems\n",
    "        - density estimation, kde\n",
    "    - Beyond linear regression (ISLR's chapters)\n",
    "    - Locally weighted regression\n",
    "        - MARS\n",
    "        - LOESS\n",
    "    - Nonparametric regression\n",
    "    - Hidden Markov and Stochastic models\n",
    "    - Naive Cause-Effect Models\n",
    "        - SEM, Path analysis\n",
    "\n",
    "**A Case Study\n",
    "\n",
    "\n",
    "20. Time Series Models\n",
    "    - Stochastic Structures\n",
    "        - Stationary, nonstationary models\n",
    "        - Random walk and stochastic terms\n",
    "        - Exploring and diagnozing TS data\n",
    "    - TS Models:\n",
    "        - Autoregression (AR)\n",
    "        - Moving Average (MA)\n",
    "        - Autoregressive Integrated Moving Average (ARIMA)\n",
    "        - Seasonal Autoregressive Integrated Moving-Average (SARIMA)\n",
    "        - Seasonal Autoregressive Integrated Moving-Average with Exogenous Regressors (SARIMAX)\n",
    "        - Vector Autoregression Moving-Average with Exogenous Regressors (VARMAX)\n",
    "        - Holt Winterâ€™s Exponential Smoothing (HWES)\n",
    "   - Long Short Term Memory (LSTM) \n",
    "\n",
    "**A Case Study\n",
    "\n",
    "\n",
    "21. NLP and Text Analyses\n",
    "    - Natural Language Processing (NLP) \n",
    "    - Text Mining \n",
    "    - Semantic analysis\n",
    "    - Topic modeling\n",
    "    - Social-media data analyses\n",
    "    - Bidirectional Encoder Representations from Transformers (BERT) \n",
    "\n",
    "**A Case Study\n",
    "\n",
    "\n",
    "22. Image Processing\n",
    "    - Classification\n",
    "    - ..\n",
    "\n",
    "**A Case Study - Steel Defect Detection \n",
    "\n",
    "\n",
    "\n",
    "**Part 7: Detect the Outliers, Reduce the Data, Use the Manifolds**\n",
    "\n",
    "\n",
    "22. Outlier Detection\n",
    "    - Statistics based methods\n",
    "    - Model-based methods\n",
    "    - GMM and kernel-based methods\n",
    "    - t-Digest\n",
    "\n",
    "\n",
    "23. Dimension Reduction Methods\n",
    "    + PC\n",
    "    + FA\n",
    "    + Clustering: row-wise and column-wise\n",
    "    + (manifolds)\n",
    "\n",
    "\n",
    "24. Manifold Learning\n",
    "    + Locally Linear Embedding (LLE)\n",
    "    + Multidimensional scaling (MDS) \n",
    "    + DBSCAN\n",
    "    + Isomap\n",
    "    + Spectral Embedding\n",
    "    + t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "    + Uniform Manifold Approximation and Projection (UMAP)\n",
    "\n",
    "\n",
    "25. HD Visualization\n",
    "    - Multivariate tools and packs\n",
    "    - Other methods\n",
    "    \n",
    "\n",
    "**A Case Study\n",
    "\n",
    "\n",
    "\n",
    "**Part 8: Evaluate and Improve the Models**\n",
    "\n",
    "\n",
    "26. Evaluation and Alternatives [https://www.jeremyjordan.me/evaluating-a-machine-learning-model/]\n",
    "    - Trade-off\n",
    "    - Data split\n",
    "    - Metrics\n",
    "    - Tools for comparing results\n",
    "    - issues\n",
    "\n",
    "\n",
    "27. Prepare Pipeline and Use Gridsearch (assuming missing data imputed)\n",
    "    + prepare pipeline on scale and models\n",
    "        - power of sci-kitlearn pack and other options\n",
    "        - validation methods\n",
    "    + use gridsearch to discover optimal situations:\n",
    "        - scaling, transformation methods\n",
    "        - models\n",
    "        - model parameters\n",
    "        - model quality scores (training, validation and testing data sets)\n",
    "        - tools for visualizing results\n",
    "    + model of everything: our pack\n",
    "        - eda+visualization+preprocessing \n",
    "        - models in the model of everything \n",
    "\n",
    "\n",
    "**A Case Study\n",
    "\n",
    "\n",
    "\n",
    "**Part 9: Estimate the Parameters with Alternative and Robust Methods**\n",
    "\n",
    "28. Parameter Estimation Methods\n",
    "    + Classicals: \n",
    "       - LS \n",
    "       - MLE \n",
    "       - Bayesian\n",
    "    + Gradient Descent and Stochastic Methods\n",
    "    + Other fast methods\n",
    "    + Hyperparameters and brief scheme to estimate\n",
    "\n",
    "\n",
    "29. Metrics, Norms and Distances\n",
    "    + Metrics, Norms\n",
    "        + L1, L2\n",
    "        + Weighted-L1\n",
    "        + Rank-based\n",
    "        + General\n",
    "    + Distances\n",
    "        + Euclidean distance\n",
    "        + Manhattan\n",
    "        + Spearman\n",
    "        + Chebyshev\n",
    "        + Jaccard\n",
    "        + Mahalanobis\n",
    "        + Cosine\n",
    "        + Bhattacharyya distance\n",
    "        + General\n",
    "\n",
    "\n",
    "**A Case Study\n",
    "\n",
    "\n",
    "\n",
    "**Part 10: Automate the Models and Present/Report the Results: Automating, Dashboard, Intelligence System, and Reproduciblity**\n",
    "\n",
    "\n",
    "30. Business Analytics and Intelligence\n",
    "    + Advanced Visualization \n",
    "    + Dashboard\n",
    "    + Smart/Recommender systems\n",
    "    + Discrete choice model\n",
    "  \n",
    "  \n",
    "31. Automating the Models\n",
    "    + generalize the pipeline and gridsearch for any data set\n",
    "    + automating\n",
    "    + real-time modeling issues\n",
    "        \n",
    "        \n",
    "32. Reproduciblity of Analyses, Report Tools, Sharing the Work\n",
    "    + Tools for Reproduciblity\n",
    "    + Writing function and automating data analyses\n",
    "    + Sharing tools\n",
    "\n",
    "**A Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **References** <a class=\"anchor\" id=\"7\"></a>\n",
    "\n",
    "Our github: https://github.com/yekabe/DataModelingStepsML/\n",
    "\n",
    "The work in this study uses the following resources:\n",
    "\n",
    "1. Notebook used from https://www.kaggle.com/prashant111/a-reference-guide-to-feature-selection-methods\n",
    "\n",
    "1. Soledad Galli's course - [Feature Selection for Machine Learning](https://www.udemy.com/course/feature-selection-for-machine-learning/)\n",
    "[How to choose the right feature selection method](#5)\n",
    "[Tips and tricks for feature selection](#6)   \n",
    "\n",
    "1. scikit-learn pack: \n",
    "      + https://scikit-learn.org/stable/user_guide.html\n",
    "      + https://scikit-learn.org/stable/auto_examples/index.html\n",
    "      + data set from https://www.kaggle.com/c/santander-customer-satisfaction/data\n",
    "\n",
    "3. [Feature Selection for Machine Learning](https://www.udemy.com/course/feature-selection-for-machine-learning/) by Soledad Galli\n",
    "\n",
    "4. [Analytics Vidhya article on Feature Selection](https://www.analyticsvidhya.com/blog/2016/12/introduction-to-feature-selection-methods-with-an-example-or-how-to-select-the-right-variables/)\n",
    "\n",
    "5. https://en.wikipedia.org/wiki/Feature_selection\n",
    "\n",
    "6. https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/\n",
    "\n",
    "\n",
    "\n",
    "# **Appendices** <a class=\"anchor\" id=\"Appndx\"></a>\n",
    "\n",
    "\n",
    "## Appendix 1. Import Packs\n",
    "\n",
    "\n",
    "+ Python packs\n",
    "        - Basics: numpy, pandas, ...\n",
    "        - Modeling: skylearn, ...\n",
    "        - Visualization: sn, plotly, ...\n",
    "        - Cloud Computing: ...\n",
    "\n",
    "\n",
    "## Appendix 2. DS Mindsets and Logics\n",
    "\n",
    "\n",
    "+ ..\n",
    "\n",
    "\n",
    "## Appendix 3. Notebook Subtopics / Topic Presentation Bulletins\n",
    "\n",
    "- Address in the Presentation:\n",
    "- What is Model?\n",
    "- What is the Theory and Math?\n",
    "- What are Parameters?\n",
    "- What are Tunings, Hyperparameters?\n",
    "- What is cost function?\n",
    "- Estimation methods / Optimization methods\n",
    "- When to use and not use the method?\n",
    "- Applications\n",
    "- Pros and Cons\n",
    "- Comparison with other methods\n",
    "- Tips\n",
    "- Performance: Computation Cost\n",
    "- The Metrics/Scores used\n",
    "- Kaggle winners on the methods\n",
    "- Small data vs Large data (n large) vs High dim data (p large)\n",
    "- Original paper + Application papers\n",
    "- References\n",
    "\n",
    "## Appendix 4. The Tips and Pitfalls in Data Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "source": [
    "\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
